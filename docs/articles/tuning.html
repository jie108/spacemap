<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Model Tuning • spacemap</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.7/flatly/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><!-- mathjax --><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">spacemap</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../installation.html">Installation</a>
</li>
<li>
  <a href="../reference/index.html">Docs</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Vignettes
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/basics.html">Model Fitting Basics</a>
    </li>
    <li>
      <a href="../articles/tuning.html">Model Tuning</a>
    </li>
    <li>
      <a href="../articles/ensemble.html">Ensemble Network</a>
    </li>
    <li>
      <a href="../articles/neta.html">Network Analysis Toolkit</a>
    </li>
  </ul>
</li>
<li>
  <a href="https://topherconley.github.io/neta-bcpls/">Application</a>
</li>
<li>
  <a href="../contact.html">Contact</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/topherconley/spacemap">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    <div class="page-header toc-ignore">
      <h1>Model Tuning</h1>
                        <h4 class="author">Christopher Conley, Pei Wang, Jie Peng</h4>
            
            <h4 class="date">2017-05-11</h4>
          </div>

    
    
<div class="contents">
<blockquote>
<p>“Harpists spend 90 percent of their lives tuning their harps and 10 percent playing out of tune.” Igor Stravinsky</p>
</blockquote>
<div id="motivation" class="section level2">
<h2 class="hasAnchor">
<a href="#motivation" class="anchor"></a>Motivation</h2>
<p>The spaceMap model learns networks in the high-dimension-low-sample-size regime by imposing a sparsity assumption on the network topology. <!--The sparsity assumption means that most nodes in the network have very few conditional dependencies among themselves relative to the potential number of edges.--> Three parameters are employed in spaceMap to control the amount of sparsity on different types of interactions, which were briefly introduced in the previous vignette <a href="https://topherconley.github.io/spacemap/articles/basics.html">Model Fitting Basics</a>. These parameters ought to be tuned to find the appropriate amount of sparsity for each data set.</p>
<!--Adding more parameters increases the range of network topologies learned by spaceMap, just as increasing the number of strings on a musical instrument can enhance the range of the artist's expression. More parameters add flexibility to the model. -->
<p>The computation effort spent on parameter tuning needs to be appropriately managed. This vignette illustrates a strategy that helps balance between computation time and network learning performance.</p>
<div id="tuning-parameters" class="section level3">
<h3 class="hasAnchor">
<a href="#tuning-parameters" class="anchor"></a>Tuning parameters</h3>
<p>We begin with describing the purpose of each tuning parameter. The first tuning parameter <span class="math inline">\(\lambda_1\)</span> controls the overall sparsity among response variable interactions <span class="math inline">\(y-y\)</span> and the second tuning parameter <span class="math inline">\(\lambda_2\)</span> controls that among the predictor-response interactions <span class="math inline">\(x-y\)</span>. Lastly, increasing the tuning parameter <span class="math inline">\(\lambda_3\)</span> will result in networks with greater representation of hub predictor nodes. <!-- possessing many interactions with response nodes---rather than reporting many predictors with very few interactions with the responses. These tuning parameters are all non-negative, where larger values impose greater sparsity in the case of $\lambda_1, \lambda_2$, or encourage $x$-hub structure in the case of $\lambda_3$.--></p>
</div>
</div>
<div id="tuning-strategy" class="section level2">
<h2 class="hasAnchor">
<a href="#tuning-strategy" class="anchor"></a>Tuning Strategy</h2>
<p>Exploring a large grid of tuning parameters can be computationally demanding. Here, we illustrate a strategy for finding a good grid of tuning parameters:</p>
<ol style="list-style-type: decimal">
<li><p>Step 1: Cross validation of SPACE model<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>, which is a special case of the spaceMap model when only <span class="math inline">\(\bf Y\)</span> input data is specified and <span class="math inline">\(\lambda_2=\lambda_3=0\)</span>. Hence, tune the SPACE model across a one-dimensional grid and identify the best performing neighborhood for <span class="math inline">\(\lambda_1\)</span>.</p></li>
<li><p>Step 2: Cross validation of spaceMap model with input data <span class="math inline">\(\bf X\)</span> and <span class="math inline">\(\bf Y\)</span> across a three-dimensional grid of <span class="math inline">\(\lambda_1, \lambda_2, \lambda_3\)</span>. The neighborhood of <span class="math inline">\(\lambda_1\)</span> is specified from step 1. Explore a broad range of values for <span class="math inline">\(\lambda_2, \lambda_3\)</span> (arguments <code>lam2</code> and <code>lam3</code>).</p></li>
<li><p>Step 3: Repeat step 2 while zooming into a smaller neighborhood of the grid if further refinement of the tuning parameters is needed.</p></li>
</ol>
</div>
<div id="tuning-example" class="section level2">
<h2 class="hasAnchor">
<a href="#tuning-example" class="anchor"></a>Tuning Example</h2>
<p>We illustrate the above strategy with an example from simulation <a href="https://topherconley.github.io/spacemap/reference/sim1.html">sim1</a>. <!-- The simulation has a known true network topology, which affords an evaluation of whether this tuning strategy leads to reasonably tuned parameter selection. 
The data has been standardized (mean-centered with unit variance) for all variables. --></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(spacemap)
<span class="kw">data</span>(sim1)</code></pre></div>
<p>Obtain the response data <span class="math inline">\(\bf Y\)</span>, a <span class="math inline">\(150 \times 171\)</span> matrix..</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Y &lt;-<span class="st"> </span>sim1$Y</code></pre></div>
<p>Obtain the predictor data <span class="math inline">\(\bf X\)</span>, a <span class="math inline">\(150 \times 14\)</span> matrix.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">X &lt;-<span class="st"> </span>sim1$X</code></pre></div>
<p>Store the dimensions and sample size.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">N &lt;-<span class="st"> </span><span class="kw">nrow</span>(X)
P &lt;-<span class="st"> </span><span class="kw">ncol</span>(X)
Q &lt;-<span class="st"> </span><span class="kw">ncol</span>(Y)</code></pre></div>
<p>Extract the true network where <span class="math inline">\(x-y\)</span> edges are stored in the <code>truth$xy</code> adjacency matrix as 1’s and <span class="math inline">\(y-y\)</span> edges are stored in the <code>truth$yy</code> adjacency matrix.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">truth &lt;-<span class="st"> </span>sim1$truth</code></pre></div>
<p>Tuning will be much faster if parallel computation is leveraged. If you choose to set up a parallel back-end (in this case for a multicore machine), it will use all available cores minus 1.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#if dopar==true, then model tuning done in parallel </span>
dopar &lt;-<span class="st"> </span><span class="ot">FALSE</span>
if (dopar) { 
  <span class="kw">library</span>(doParallel)
  <span class="kw">library</span>(parallel)
  ncores &lt;-<span class="st"> </span><span class="kw">detectCores</span>()  -<span class="st"> </span><span class="dv">1</span>
  cl &lt;-<span class="st"> </span><span class="kw">makeCluster</span>(ncores)
  <span class="kw">registerDoParallel</span>(cl)
}</code></pre></div>
<div id="step-1-find-a-neighborhood-for-lam1" class="section level3">
<h3 class="hasAnchor">
<a href="#step-1-find-a-neighborhood-for-lam1" class="anchor"></a>Step 1: find a neighborhood for <code>lam1</code>
</h3>
<p>Tune <code>lam1</code> by fitting the SPACE model<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> to <span class="math inline">\(Y\)</span> over a one-dimensional tuning grid. <!--In the current implementation, the tuning parameter scales with the sample size--> We use a result from Meinshausen and Buhlmann (2006)<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> to initialize <span class="math inline">\(\lambda_{1}\)</span><a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#initialize lam1 according to Meinshausen and Buhlmann</span>
lam1start &lt;-<span class="st"> </span>function(n, q, alpha) { 
  <span class="kw">sqrt</span>(n) *<span class="st"> </span><span class="kw">qnorm</span>(<span class="dv">1</span> -<span class="st"> </span>(alpha/<span class="st"> </span>(<span class="dv">2</span>*q^<span class="dv">2</span>)))
}
<span class="co">#value of alpha is meant to control the false discovery rate</span>
<span class="co">#in our experience  alpha should be set very conservatively </span>
<span class="co">#to obtain an initial value closer to the CV-selected lam1. </span>
lam0 &lt;-<span class="st"> </span><span class="kw">lam1start</span>(<span class="dt">n =</span> <span class="kw">floor</span>(N -<span class="st"> </span>N*.<span class="dv">10</span>), <span class="dt">q =</span> Q, <span class="dt">alpha =</span> <span class="fl">1e-5</span>)
lam0</code></pre></div>
<pre><code>## [1] 72.94889</code></pre>
<p>Take the initial grid search for <code>lam1</code> to range from [80% of <code>lam0</code>, 120% of <code>lam0</code>].</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#initial grid size. </span>
ngrid &lt;-<span class="st"> </span><span class="dv">30</span>
<span class="co">#80% of lam0</span>
eps1 &lt;-<span class="st"> </span><span class="fl">0.8</span>
<span class="co">#grid should be a data.frame</span>
tsp &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">lam1 =</span> <span class="kw">seq</span>(lam0*eps1, lam0*(<span class="dv">1</span> +<span class="st"> </span>(<span class="dv">1</span> -<span class="st"> </span>eps1)), <span class="dt">length =</span> ngrid))
<span class="kw">summary</span>(tsp)</code></pre></div>
<pre><code>##       lam1      
##  Min.   :58.36  
##  1st Qu.:65.65  
##  Median :72.95  
##  Mean   :72.95  
##  3rd Qu.:80.24  
##  Max.   :87.54</code></pre>
<p>In preparation of cross validation, we encourage the user to determine the split of the data by themselves since the data may have some special underlying population structure that needs to be balanced across the hold-out sets. Below we illustrate one way of splitting the data through the <code>caret</code> R package.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#for generating cross-validation folds</span>
<span class="kw">library</span>(caret)
<span class="co">#number of folds</span>
K &lt;-<span class="st"> </span>10L
<span class="kw">set.seed</span>(265616L)
<span class="co">#no special population structure, but create randomized dummy structure of A and B</span>
testSets &lt;-<span class="st"> </span><span class="kw">createFolds</span>(<span class="dt">y =</span> <span class="kw">sample</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="st">"A"</span>, <span class="st">"B"</span>), <span class="dt">size =</span> N, <span class="dt">replace =</span> <span class="ot">TRUE</span>), <span class="dt">k =</span> K)
trainSets &lt;-<span class="st"> </span><span class="kw">lapply</span>(testSets, function(s) <span class="kw">setdiff</span>(<span class="kw">seq_len</span>(N), s))</code></pre></div>
<p>Conduct the cross-validation through the <a href="https://topherconley.github.io/spacemap/reference/cvVote.html">cvVote</a> function by specifying <code>method = "space"</code>. Also input the data <span class="math inline">\(\bf Y\)</span>, the lists of test and training sample splits, and a grid for the tuning parameter.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cvspace &lt;-<span class="st"> </span><span class="kw"><a href="../reference/cvVote.html">cvVote</a></span>(<span class="dt">Y =</span> Y, 
                  <span class="dt">trainIds =</span> trainSets, <span class="dt">testIds =</span> testSets, 
                  <span class="dt">method =</span> <span class="st">"space"</span>, <span class="dt">tuneGrid =</span> tsp) </code></pre></div>
<p>The CV-selected <span class="math inline">\(\lambda^*_1\)</span> is reported as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">minLam1 &lt;-<span class="st"> </span>cvspace$minTune$lam1
minLam1</code></pre></div>
<pre><code>## [1] 65.40245</code></pre>
<p>The <span class="math inline">\(y-y\)</span> edges are stored in the adjacency matrix <code>cvspace$cvVote$yy</code>. The number of <span class="math inline">\(y-y\)</span> edges is reported as:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">nyy &lt;-<span class="st"> </span><span class="kw"><a href="../reference/nonZeroUpper.html">nonZeroUpper</a></span>(cvspace$cvVote$yy,<span class="dv">0</span>)
nyy</code></pre></div>
<pre><code>## [1] 162</code></pre>
<p>The <a href="https://topherconley.github.io/spacemap/reference/tuneVis.html">tuneVis</a> function performs diagnostics stored as a list:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#the size of each test set</span>
nsplits &lt;-<span class="st"> </span><span class="kw">sapply</span>(testSets, length)
<span class="co">#required for plots</span>
<span class="kw">library</span>(ggplot2)
cvVis &lt;-<span class="st"> </span><span class="kw"><a href="../reference/tuneVis.html">tuneVis</a></span>(<span class="dt">cvOut =</span> cvspace, 
                 <span class="dt">testSetLen =</span> nsplits, 
                 <span class="dt">tuneParam1 =</span> tsp$lam1,
                 <span class="dt">tuneParam1Name =</span> <span class="st">"lam1"</span>)</code></pre></div>
<!--In this tuning step, we only need the first and third  diagnostic plots.-->
<p>Below the plot on the left shows the log(CV score) versus <code>lam1</code> where the vertical line denotes the minimizer. <!--The CV score curve has an approximately convex shape with a little bit variability at the valley. --> On the right is the average number of <span class="math inline">\(y-y\)</span> edges across CV training splits which is decreasing with increasing <code>lam1</code>. The intersecting lines indicate that the optimal lambda produces a CV.vote model of 162 <span class="math inline">\(y-y\)</span> edges, which is about 15 edges less than the average no. of edges due to the stabilizing effect of the CV.vote procedure.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#for combining plots</span>
<span class="kw">library</span>(gridExtra)
<span class="co">#geom_vline/hline for vertical/horizontal lines</span>
<span class="kw">grid.arrange</span>(cvVis[[<span class="dv">1</span>]] +<span class="st"> </span>
<span class="st">               </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> minLam1),
             cvVis[[<span class="dv">3</span>]] +<span class="st"> </span>
<span class="st">               </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> nyy) +<span class="st"> </span>
<span class="st">               </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span>  minLam1)
             , <span class="dt">ncol =</span> <span class="dv">2</span>)</code></pre></div>
<p><img src="tuning_files/figure-html/unnamed-chunk-15-1.png" width="672"></p>
<p>Since the true network is known, we can evaluate the learned SPACE network against the truth.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">spacePerf &lt;-<span class="st"> </span><span class="kw"><a href="../reference/cvPerf.html">cvPerf</a></span>(<span class="dt">cvOut =</span> cvspace, <span class="dt">truth =</span> truth, <span class="dt">method =</span> <span class="st">"space"</span>)[<span class="dv">1</span>:<span class="dv">3</span>]
spacePerf</code></pre></div>
<pre><code>##     power       fdr       mcc 
## 0.7820513 0.2469136 0.7648620</code></pre>
<p>The “mcc” measure combines the power and FDR. In this case, the power (or sensitivity) is reasonable, but the FDR (or specificity) is a little high. Next, we will see the effect of conditioning on <span class="math inline">\(\bf X\)</span> through the spaceMap model.</p>
</div>
<div id="step-2-find-a-neighborhood-for-lam2-and-lam3" class="section level3">
<h3 class="hasAnchor">
<a href="#step-2-find-a-neighborhood-for-lam2-and-lam3" class="anchor"></a>Step 2: find a neighborhood for <code>lam2</code> and <code>lam3</code>
</h3>
<p>Now shifting our attention to tuning all three parameters.</p>
<p>Here we make use of the information from step 1 to reduce the grid search time. Input a neighborhood of <span class="math inline">\(65 \leq \lambda_1 \leq 75\)</span> into the 3-D grid since we know that produces a reasonable output for the <span class="math inline">\(y-y\)</span> part of the network from step 1.</p>
<p>The initial neighborhood for <span class="math inline">\(\lambda_2, \lambda_3\)</span> is guided by practical considerations such as the maximum number of <span class="math inline">\(x-y\)</span> edges one might expect. In this case, since there are only 14 <span class="math inline">\(x\)</span> variables, we do not expect there to be more than say, 200 <span class="math inline">\(x-y\)</span> edges, but no less than 50 <span class="math inline">\(x-y\)</span> edges in a sparse network. When <span class="math inline">\(P \leq Q\)</span> we have noticed in our experience that the optimal <span class="math inline">\(\lambda^*_1\)</span> from step 1 serves as a good upper bound for initializing <span class="math inline">\(\lambda_2,\lambda_3\)</span>, although this may not always be the case. Also, our experience suggests that the network size is more sensitive to <span class="math inline">\(\lambda_2\)</span> than to <span class="math inline">\(\lambda_3\)</span>; therefore, finding an appropriate neighborhood for <span class="math inline">\(\lambda_2\)</span> is more critical.</p>
<p>In the following, take <span class="math inline">\(15 \leq \lambda_2 \leq 60 (&lt; \lambda^*_1)\)</span> and fix <span class="math inline">\(\lambda_3=15 &gt; 0\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#define grid</span>
tmap1 &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">lam1 =</span> minLam1, 
                     <span class="dt">lam2 =</span> <span class="kw">seq</span>(<span class="dv">15</span>, <span class="dv">60</span>, <span class="dt">by =</span> <span class="dv">5</span>), 
                     <span class="dt">lam3 =</span> <span class="dv">15</span>)</code></pre></div>
<p>We can obtain an approximate lower bound for <span class="math inline">\(\lambda_2\)</span> by applying the <a href="https://topherconley.github.io/spacemap/reference/initFit.html">initFit</a> function to get a sense of the number of <span class="math inline">\(x-y\)</span> edges (or <span class="math inline">\(y-y\)</span> edges) for a specific tuning set. This function does not do any cross-validation, but simply fits the model once for each tuning parameter combination in the grid.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ntmap1 &lt;-<span class="st"> </span><span class="kw"><a href="../reference/initFit.html">initFit</a></span>(<span class="dt">Y =</span> Y, <span class="dt">X =</span> X, <span class="dt">method =</span> <span class="st">"spacemap"</span>, <span class="dt">tuneGrid =</span> tmap1)
<span class="kw">library</span>(ggplot2)
<span class="kw">qplot</span>(<span class="dt">x =</span> tmap1$lam2, <span class="dt">y =</span> ntmap1$nxy,
      <span class="dt">ylab =</span> <span class="st">"No. of X-&gt;Y edges"</span>, <span class="dt">xlab =</span> <span class="st">"lam2"</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">theme_bw</span>()</code></pre></div>
<p><img src="tuning_files/figure-html/unnamed-chunk-18-1.png" width="672"></p>
<p>The above plot suggests restricting our attention to <span class="math inline">\(20\leq \lambda_2 \leq 35\)</span> to limit the number of <span class="math inline">\(x-y\)</span> edges to the range that we are targeting.</p>
<p>Now that we have reasonable ranges for <span class="math inline">\(\lambda_1,\lambda_2\)</span>, we also need to find an appropriate range for <span class="math inline">\(\lambda_3\)</span>. We could use the <a href="https://topherconley.github.io/spacemap/reference/initFit.html">initFit</a> function to guide this process; however, since we expect that <span class="math inline">\(x\)</span>-hubs are likely to exist in the network, the lower bound for <span class="math inline">\(\lambda_3\)</span> should be sufficiently far away from 0.</p>
<p>Now define the 3-D tuning grid:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tmap2 &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">lam1 =</span> <span class="kw">seq</span>(<span class="dv">65</span>, <span class="dv">75</span>, <span class="dt">length =</span> <span class="dv">5</span>), 
                    <span class="dt">lam2 =</span> <span class="kw">seq</span>(<span class="dv">21</span>, <span class="dv">35</span>, <span class="dt">length =</span> <span class="dv">5</span>), 
                    <span class="dt">lam3 =</span> <span class="kw">seq</span>(<span class="dv">10</span>, <span class="dv">40</span>, <span class="dt">length =</span> <span class="dv">5</span>))</code></pre></div>
<p>Apply cross validation with <code>method = "spacemap"</code> to learn the optimal network in the tuning grid. For this data set, this will take about 4 minutes on a single processor.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cvsmap &lt;-<span class="st"> </span><span class="kw"><a href="../reference/cvVote.html">cvVote</a></span>(<span class="dt">Y =</span> Y, <span class="dt">X =</span> X, 
                 <span class="dt">trainIds =</span> trainSets, <span class="dt">testIds =</span> testSets, 
                 <span class="dt">method =</span> <span class="st">"spacemap"</span>, <span class="dt">tuneGrid =</span> tmap2)</code></pre></div>
<p>The CV.vote spaceMap network is encoded into adjacency matrices <code>cvsmap$cvVote[c("xy", "yy")]</code>. The optimal tuning parameters from the tuning grid are:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cvsmap$minTune</code></pre></div>
<pre><code>## $lam1
## [1] 70
## 
## $lam2
## [1] 28
## 
## $lam3
## [1] 17.5</code></pre>
<p>Next we diagnose the suitability of the tuning grid based on the output through <code>tuneVis</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cvVis1 &lt;-<span class="st"> </span>spacemap::<span class="kw"><a href="../reference/tuneVis.html">tuneVis</a></span>(<span class="dt">cvOut =</span> cvsmap, <span class="dt">testSetLen =</span> nsplits, 
                            <span class="dt">tuneParam1 =</span> tmap2$lam1, <span class="dt">tuneParam1Name =</span> <span class="st">"lam1"</span>)
cvVis2 &lt;-<span class="st"> </span>spacemap::<span class="kw"><a href="../reference/tuneVis.html">tuneVis</a></span>(<span class="dt">cvOut =</span> cvsmap, <span class="dt">testSetLen =</span> nsplits, 
                            <span class="dt">tuneParam1 =</span> tmap2$lam2, <span class="dt">tuneParam1Name =</span> <span class="st">"lam2"</span>)
cvVis3 &lt;-<span class="st"> </span>spacemap::<span class="kw"><a href="../reference/tuneVis.html">tuneVis</a></span>(<span class="dt">cvOut =</span> cvsmap, <span class="dt">testSetLen =</span> nsplits, 
                            <span class="dt">tuneParam1 =</span> tmap2$lam3, <span class="dt">tuneParam1Name =</span> <span class="st">"lam3"</span>)</code></pre></div>
<p>Visualizing the CV scores across the 3-D grid shows that the selected tuning parameters are away from their respective boundaries, which suggests the grid is suitable. If a selected parameter is on or near the boundary, it suggests that we may increase (or decrease) the upper (or lower) bound for that parameter on the grid.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(gridExtra)
<span class="kw">grid.arrange</span>(cvVis1[[<span class="dv">1</span>]], cvVis2[[<span class="dv">1</span>]],cvVis3[[<span class="dv">1</span>]], <span class="dt">ncol =</span> <span class="dv">2</span>)</code></pre></div>
<p><img src="tuning_files/figure-html/unnamed-chunk-23-1.png" width="672"></p>
<p>This whole tuning process took about 6 minutes on a single processor. The performance of the final network is listed below</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">smapPerf &lt;-<span class="st"> </span><span class="kw"><a href="../reference/cvPerf.html">cvPerf</a></span>(<span class="dt">cvOut =</span> cvsmap, <span class="dt">truth =</span> truth, <span class="dt">method =</span> <span class="st">"spacemap"</span>)[<span class="dv">1</span>:<span class="dv">8</span>]
smapPerf</code></pre></div>
<pre><code>##      power        fdr        mcc    powerXY      fdrXY    powerYY 
## 0.76666667 0.07471264 0.84049251 0.87037037 0.07843137 0.73076923 
##      fdrYY      mccYY 
## 0.07317073 0.82133452</code></pre>
<p>Note that, the spaceMap model dramatically lowers the <span class="math inline">\(y-y\)</span> FDR when compared with the SPACE model with only a small drop in power. <!--The FDR is lowered from  over 25% to below 8%, while experiencing a small drop in $y-y$ power. This benefit is believed to be because spaceMap learns the $x-y$ edges that encode predictor variable perturbations to the response variables.  With a relatively high $x-y$ power and low overall FDR, spaceMap is a real improvement upon SPACE.--></p>
</div>
<div id="step-3-optional-refinement-of-the-grid" class="section level3">
<h3 class="hasAnchor">
<a href="#step-3-optional-refinement-of-the-grid" class="anchor"></a>Step 3 (optional): refinement of the grid</h3>
<p>Further refinement could be explored by narrowing the neighborhood around the previously found optimal tuning set and using a finer grid. <!--However, note that refining the grid may lead to a better CV score, but worse performance due to over-fitting.--></p>
</div>
</div>
<div id="further-reading" class="section level2">
<h2 class="hasAnchor">
<a href="#further-reading" class="anchor"></a>Further Reading</h2>
<p>This vignette discusses how to choose tuning parameters for the spaceMap model. We recommend looking at the <a href="https://topherconley.github.io/spacemap/articles/ensemble.html">next vignette</a> which illustrates how to further control FDR through an ensemble of networks learned on bootstrap replicates of the data.</p>
</div>
<div class="footnotes">
<hr>
<ol>
<li id="fn1"><p>Peng, Wang, Zhou and Zhu (2009). Partial Correlation Estimation by Joint Sparse Regression Models, Journal of the American Statistical Association , Vol. 104, No. 486, 735-746<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Peng, Wang, Zhou and Zhu (2009). Partial Correlation Estimation by Joint Sparse Regression Models, Journal of the American Statistical Association , Vol. 104, No. 486, 735-746<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>Meinshausen, Nicolai; Bühlmann, Peter. High-dimensional graphs and variable selection with the Lasso. Ann. Statist. 34 (2006), no. 3, 1436–1462. <a href="doi:10.1214/009053606000000281" class="uri">doi:10.1214/009053606000000281</a><a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>applies when <span class="math inline">\(\bf Y\)</span> has been standardized to have unit variance for all <span class="math inline">\(Q\)</span> variables<a href="#fnref4">↩</a></p></li>
</ol>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#motivation">Motivation</a></li>
      <li><a href="#tuning-strategy">Tuning Strategy</a></li>
      <li><a href="#tuning-example">Tuning Example</a></li>
      <li><a href="#further-reading">Further Reading</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by <a href="https://github.com/topherconley/">Christopher Conley</a>, <a href="http://research.mssm.edu/wanglab/index.htm">Pei Wang</a>, <a href="http://www.stat.ucdavis.edu/~jie/">Jie Peng</a>, UC Davis.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  </body>
</html>
