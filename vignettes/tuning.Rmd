---
title: "Model Tuning"
author: "Christopher Conley, Pei Wang, Jie Peng"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    fig_caption: yes
vignette: >
  %\VignetteIndexEntry{Model Tuning}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, echo = FALSE}
library(knitr)
opts_chunk$set(message=F, warning=F)
ptm <- proc.time()
```


> "Harpists spend 90 percent of their lives tuning their harps and 10 percent playing out of tune."
Igor Stravinsky

## Motivation

The spaceMap model learns networks in the high-dimension-low-sample-size regime by imposing a sparsity assumption on the network topology. <!--The sparsity assumption means that most nodes in the network have very few conditional dependencies among themselves relative to the potential number of edges.--> 
Three parameters are employed in spaceMap to  control the amount of sparsity on different types of interactions, which were briefly introduced in the previous vignette [Model Fitting Basics](https://topherconley.github.io/spacemap/articles/basics.html). 
These parameters ought to be tuned to find the appropriate amount of sparsity  for each data set. 

<!--Adding more parameters increases the range of network topologies learned by spaceMap, just as increasing the number of strings on a musical instrument can enhance the range of the artist's expression. More parameters add flexibility to the model. -->
The computation effort spent on parameter tuning needs to be appropriately managed. This vignette illustrates a strategy that helps balance between computation time and network learning performance. 

### Tuning parameters

We begin with describing the  purpose of each tuning parameter. The first tuning parameter $\lambda_1$ controls the overall sparsity  among response variable interactions $y-y$ and the second tuning parameter $\lambda_2$ controls that among the predictor-response interactions $x-y$. Lastly, increasing the tuning parameter $\lambda_3$ will result in networks with greater representation of hub predictor nodes. <!-----possessing many interactions with response nodes---rather than reporting many predictors with very few interactions with the responses. These tuning parameters are all non-negative, where larger values impose greater sparsity in the case of $\lambda_1, \lambda_2$, or encourage $x$-hub structure in the case of $\lambda_3$. -->

## Tuning Strategy

Exploring a large grid of tuning parameters can be computationally demanding. Here, we illustrate a strategy for finding a good neighborhood of tuning parameters:

1. Cross validation of SPACE model^[Peng, Wang, Zhou and Zhu (2009). Partial Correlation Estimation by Joint Sparse Regression Models, Journal of the American Statistical Association , Vol. 104, No. 486, 735-746], which is a special case of the spaceMap model when only $\bf Y$ input data is specified and $\lambda_2=\lambda_3=0$. Hence, tune the SPACE model across a one-dimensional grid of tuning parameter $\lambda_1$ (argument `lam1`) and  identify best performing neighborhood of $\lambda_1$. 

2. Cross validation of spaceMap model with input data $\bf X$ and $\bf Y$ across a three-dimensional grid of $\lambda_1, \lambda_2, \lambda_3$. The neighborhood of $\lambda_1$ is specified from step 1. Explore a broad range of values for $\lambda_2, \lambda_3$ (arguments `lam2` and `lam3`).  

3. Repeat step 2 if further refinement of the tuning parameters is needed. 

## Tuning Example 

We illustrate the above strategy with an example from simulation [sim1](https://topherconley.github.io/spacemap/reference/sim1.html). <!-- The simulation has a known true network topology, which affords an evaluation of whether this tuning strategy leads to reasonably tuned parameter selection. 
The data has been standardized (mean-centered with unit variance) for all variables. -->

```{r}
library(spacemap)
data(sim1)
```

Obtain the response data $\bf Y$, an $150 \times 171$ matrix..

```{r}
Y <- sim1$Y
```

Obtain the predictor data $\bf X$, an $150 \times 14$ matrix.

```{r}
X <- sim1$X
```

Store the dimensions and sample size.

```{r}
N <- nrow(X)
P <- ncol(X)
Q <- ncol(Y)
```

Extract the true network where $x-y$ edges  are stored in the `truth$xy` adjacency matrix as  1's and $y-y$ edges are stored in the `truth$yy` adjacency matrix.  

```{r}
truth <- sim1$truth
```

Tuning will be much faster if parallel computation is leveraged. If you choose to set up a parallel back-end (in this case for a multicore machine), it will use all available cores minus 1. 

```{r}
#if dopar==true, then model tuning done in parallel 
dopar <- FALSE
if (dopar) { 
  library(doParallel)
  library(parallel)
  ncores <- detectCores()  - 1
  cl <- makeCluster(ncores)
  registerDoParallel(cl)
}
```

### Step 1: find a neighborhood for `lam1`

Tune the `lam1` parameter by fitting the SPACE model^[Peng, Wang, Zhou and Zhu (2009). Partial Correlation Estimation by Joint Sparse Regression Models, Journal of the American Statistical Association , Vol. 104, No. 486, 735-746] to $Y$ over a one-dimensional tuning grid. In the current implementation, the tuning parameter scales with the sample size using a result from Meinshausen and Buhlmann (2006)^[Meinshausen, Nicolai; BÃ¼hlmann, Peter. High-dimensional graphs and variable selection with the Lasso. Ann. Statist. 34 (2006), no. 3, 1436--1462. doi:10.1214/009053606000000281] to initialize $\lambda_{1}$^[applies when $\bf Y$ has been standardized to have unit variance for all $Q$ variables]. 

```{r}
#initialize lam1 according to Meinshausen and Buhlmann
lam1start <- function(n, q, alpha) { 
  sqrt(n) * qnorm(1 - (alpha/ (2*q^2)))
}
#value of alpha is meant to control the false discovery rate
#in our experience  alpha should be set very conservatively 
#to obtain an initial value closer to the CV-selected lam1. 
lam0 <- lam1start(n = floor(N - N*.10), q = Q, alpha = 1e-5)
lam0
```

Take the initial grid search for `lam1` to range from  [80% of `lam0`, 120% of `lam0`]. 

```{r}
#initial grid size. 
ngrid <- 30
#80% of lam0
eps1 <- 0.8
#grid should be a data.frame
tsp <- expand.grid(lam1 = seq(lam0*eps1, lam0*(1 + (1 - eps1)), length = ngrid))
summary(tsp)
```

In preparation of cross validation, we encourage the user to determine the split of the data by themselves since the data may have some special underlying population structure that needs to be balanced across the hold-out sets. Below we illustrate one means of splitting the data through the external `caret` R package. 

```{r}
#for generating cross-validation folds
library(caret)
#number of folds
K <- 10L
set.seed(265616L)
#no special population structure, but create randomized dummy structure of A and B
testSets <- createFolds(y = sample(x = c("A", "B"), size = N, replace = TRUE), k = K)
trainSets <- lapply(testSets, function(s) setdiff(seq_len(N), s))
```

Conduct the cross-validation through the [cvVote](https://topherconley.github.io/spacemap/reference/cvVote.html) function by specifying `method = "space"`. Also input the data $\bf Y$, the lists of test and training sample splits, and a grid for the  tuning parameter. 

```{r}
cvspace <- cvVote(Y = Y, 
                  trainIds = trainSets, testIds = testSets, 
                  method = "space", tuneGrid = tsp) 
```

The CV-selected $\lambda^*_1$ is reported as follows: 

```{r}
minLam1 <- cvspace$minTune$lam1
minLam1
```


The $y-y$ edges are stored in the adjacency matrix `cvspace$cvVote$yy`. The number of $y-y$ edges is reported as: 

```{r}
nyy <- nonZeroUpper(cvspace$cvVote$yy,0)
nyy
```


The [tuneVis](https://topherconley.github.io/spacemap/reference/tuneVis.html) function generates several diagnostic, which are stored in a list: 

```{r}
#the size of each test set
nsplits <- sapply(testSets, length)
#required for plots
library(ggplot2)
cvVis <- tuneVis(cvOut = cvspace, 
                 testSetLen = nsplits, 
                 tuneParam1 = tsp$lam1,
                 tuneParam1Name = "lam1")
```

<!--In this tuning step, we only need the first and third  diagnostic plots.--> Below the plot on the left shows the log(CV score) versus `lam1` where the vertical line denotes the minimizer. <!--The CV score curve has an approximately convex shape with a little bit variability at the valley. --> On the right is the average number of $y-y$ edges across CV training splits which is  decreasing with increasing `lam1`. The intersecting lines indicate that the optimal lambda produces a CV.vote model of `r nonZeroUpper(cvspace$cvVote$yy,0)` $y-y$ edges, which is about 15 edges less than the average no. of edges due to the stabilizing effect of the CV.vote procedure. 

```{r}
#for combining plots
library(gridExtra)
#geom_vline/hline for vertical/horizontal lines
grid.arrange(cvVis[[1]] + 
               geom_vline(xintercept = minLam1),
             cvVis[[3]] + 
               geom_hline(yintercept = nyy) + 
               geom_vline(xintercept =  minLam1)
             , ncol = 2)
```

Since the true network is known, we can evaluate the learned SPACE network against the truth. 

```{r}
spacePerf <- cvPerf(cvOut = cvspace, truth = truth, method = "space")[1:3]
spacePerf
```

The "mcc" measure combines the power and FDR. In this case, the power (or sensitivity) is reasonable, but the FDR (or specificity) is a little high. Next, we will see the effect of conditioning on $\bf X$ through the spaceMap model. 

### Step 2: find a neighborhood for `lam2` and `lam3`

Now shifting our attention to tuning all three parameters. Here we can make use of the information from step 1 to reduce the grid search time. 

Input a neighborhood of $65 \leq \lambda_1 \leq 75$ into the 3-D grid since we know that produces a reasonable output for the $y-y$ portion of the network.
The initial neighborhood for $\lambda_2, \lambda_3$ is guided by practical considerations such as the maximum number of  $x-y$ edges one might expect. In this case, since there are only 14 $x$ variables, we do not expect there to be more than say, 200 $x-y$ edges, but no less than 50 $x-y$ edges in a sparse network.
When $P \leq Q$ we have noticed in our experience that $\lambda^*_1$ from step 1 serves as a good upper bound for initializing $\lambda_2,\lambda_3$, although this may not always be the case. Also, our experience suggests that the network size is more sensitive to $\lambda_2$ than to $\lambda_3$; therefore, finding an appropriate neighborhood for $\lambda_2$ is most critical. 


In the following, take $15 \leq \lambda_2 \leq 60 (< \lambda^*_1)$ and fix $\lambda_3=15 > 0$: 

```{r}
#define grid
tmap1 <- expand.grid(lam1 = minLam1, 
                     lam2 = seq(15, 60, by = 5), 
                     lam3 = 15)
```

We can  obtain an approximate lower bound for $\lambda_2$ by applying the [initFit](https://topherconley.github.io/spacemap/reference/initFit.html) function to get a sense of the number of $x-y$ edges (or $y-y$) for a specific tuning set. This function does not do any cross-validation, but simply fits the model once for each tuning parameter combination in the grid. 

```{r}
ntmap1 <- initFit(Y = Y, X = X, method = "spacemap", tuneGrid = tmap1)
library(ggplot2)
qplot(x = tmap1$lam2, y = ntmap1$nxy,
      ylab = "No. of X->Y edges", xlab = "lam2") + 
  theme_bw()
```

The above plot suggests restricting our attention to $20\leq \lambda_2 \leq 35$ to limit the number of $x-y$ edges to the range that we are targeting. 

Now that we have reasonable ranges for $\lambda_1,\lambda_2$, we also need to find an appropriate range for $\lambda_3$. We could use the [initFit](https://topherconley.github.io/spacemap/reference/initFit.html) function to guide this process; however, since we expect  $x$-hubs likely to exist in the network, the lower bound for $\lambda_3$ should be sufficiently far away from 0. 


Now define the 3-D tuning grid: 

```{r}
tmap2 <- expand.grid(lam1 = seq(65, 75, length = 5), 
                    lam2 = seq(21, 35, length = 5), 
                    lam3 = seq(10, 40, length = 5))
```

Apply cross validation with `method = "spacemap"` to learn the optimal network in the tuning grid. For this data set, this will take about 4 minutes on a single processor. 

```{r}
cvsmap <- cvVote(Y = Y, X = X, 
                 trainIds = trainSets, testIds = testSets, 
                 method = "spacemap", tuneGrid = tmap2)
```

The CV.vote spaceMap network is encoded into adjacency matrices `cvsmap$cvVote[c("xy", "yy")]`. The optimal tuning parameters from the tuning grid are: 

```{r}
cvsmap$minTune
```

Next we diagnose the suitability of the tuning grid based on the output through `tuneVis`.  

```{r}
cvVis1 <- spacemap::tuneVis(cvOut = cvsmap, testSetLen = nsplits, 
                            tuneParam1 = tmap2$lam1, tuneParam1Name = "lam1")
cvVis2 <- spacemap::tuneVis(cvOut = cvsmap, testSetLen = nsplits, 
                            tuneParam1 = tmap2$lam2, tuneParam1Name = "lam2")
cvVis3 <- spacemap::tuneVis(cvOut = cvsmap, testSetLen = nsplits, 
                            tuneParam1 = tmap2$lam3, tuneParam1Name = "lam3")
```

Visualizing the CV scores across the 3-D grid shows that the selected tuning parameters are away from their respective boundaries, which suggests the grid is suitable. If the selected parameter was on or near the boundary, it may suggest that we should increase (or decrease) the upper (or lower) bound for that parameter on the grid. 

```{r}
library(gridExtra)
grid.arrange(cvVis1[[1]], cvVis2[[1]],cvVis3[[1]], ncol = 2)
```

This whole tuning process took about `r round((proc.time() - ptm)[3]/60, 0)` minutes  on a single processor. The performance of the final network is listed below

```{r}
smapPerf <- cvPerf(cvOut = cvsmap, truth = truth, method = "spacemap")[1:8]
smapPerf
```

The spaceMap model dramatically lowers the $y-y$ FDR when compared with the SPACE model with only a small drop in power. <!--The FDR is lowered from  over 25% to below 8%, while experiencing a small drop in $y-y$ power. This benefit is believed to be because spaceMap learns the $x-y$ edges that encode predictor variable perturbations to the response variables.  With a relatively high $x-y$ power and low overall FDR, spaceMap is a real improvement upon SPACE.--> 

### Step 3 (optional): refinement of the grid

Further refinement could be explored by narrowing the neighborhood around the previously optimal tuning set and using a finer grid. <!--However, note that refining the grid may lead to a better CV score, but worse performance due to over-fitting.--> 


```{r, echo  = FALSE, eval = FALSE}
tmap3 <- expand.grid(lam1 = cvsmap$minTune$lam1, 
                    lam2 = seq(cvsmap$minTune$lam2 - 3, 
                               cvsmap$minTune$lam2 + 3, length = 5), 
                    lam3 = seq(cvsmap$minTune$lam3 - 5, 
                               cvsmap$minTune$lam3 + 5, length = 5))
#Perform cross validation again with the refined grid. 

cvsmap2 <- cvVote(Y = Y, X = X, 
                  trainIds = trainSets, testIds = testSets, 
                  method = "spacemap", tuneGrid = tmap3)
#Compare which CV score is lower between the two cross validation runs. 

cvsmap$logcvScore
cvsmap2$logcvScore
round((proc.time() - ptm)[3]/60, 2)

spacemap::cvPerf(cvOut = cvsmap, truth = truth, method = "spacemap")[1:8]
spacemap::cvPerf(cvOut = cvsmap2, truth = truth, method = "spacemap")[1:8]
```


## Further Reading 

This vignette discusses how to choose tuning parameters for the spaceMap model. We recommend looking at the [next vignette](https://topherconley.github.io/spacemap/articles/ensemble.html) which illustrates how to further control FDR through an ensemble of networks learned on bootstrap replicates of the data. 

